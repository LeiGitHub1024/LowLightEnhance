{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenoiseAutoEncoder(\n",
       "  (Gamma): Sequential(\n",
       "    (0): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=285180, out_features=50, bias=True)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
       "    (10): Sigmoid()\n",
       "  )\n",
       "  (Encoder1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU()\n",
       "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Decoder1): Sequential(\n",
       "    (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ConvTranspose2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): Sigmoid()\n",
       "  )\n",
       "  (Encoder2): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU()\n",
       "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Decoder2): Sequential(\n",
       "    (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ConvTranspose2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# from model.autoEncoder import DenoiseAutoEncoder\n",
    "\n",
    "#这里很奇怪，直接引用模型会出错。得抄过来......\n",
    "class DenoiseAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenoiseAutoEncoder, self).__init__()\n",
    "        # Encoder1\n",
    "        self.Gamma = nn.Sequential(\n",
    "            nn.Conv2d(3, 10, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(10, 20, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(285180, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.Encoder1 = nn.Sequential(\n",
    "            # param [input_c, output_c, kernel_size, stride, padding]\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),   # [, 64, 96, 96]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1), # [, 64, 96, 96]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),             # [, 64, 48, 48]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1),  # [, 64, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), # [, 128, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1), # [, 128, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [, 256, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                 # [, 256, 24, 24]\n",
    "            nn.BatchNorm2d(256)   \n",
    "        )\n",
    "        \n",
    "        # decoder1\n",
    "        self.Decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 3 ,1, 1),   # [, 128, 24, 24]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ConvTranspose2d(128, 128, 3, 2, 1, 1),   # [, 128, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ConvTranspose2d(128, 64, 3, 1, 1),    # [, 64, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ConvTranspose2d(64, 32, 3, 1, 1),      # [, 32, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ConvTranspose2d(32, 32, 3, 1, 1),      # [, 32, 48, 48]\n",
    "            nn.ConvTranspose2d(32, 16, 3, 2, 1, 1),  # [, 16, 96, 96]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ConvTranspose2d(16, 3, 3, 1, 1),         # [, 3, 96, 96]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "         # Encoder2\n",
    "        self.Encoder2 = nn.Sequential(\n",
    "            # param [input_c, output_c, kernel_size, stride, padding]\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),   # [, 64, 96, 96]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1), # [, 64, 96, 96]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),             # [, 64, 48, 48]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1),  # [, 64, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), # [, 128, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1), # [, 128, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [, 256, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                 # [, 256, 24, 24]\n",
    "            nn.BatchNorm2d(256)   \n",
    "        )\n",
    "        \n",
    "        # decoder2\n",
    "        self.Decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 3 ,1, 1),   # [, 128, 24, 24]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ConvTranspose2d(128, 128, 3, 2, 1, 1),   # [, 128, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ConvTranspose2d(128, 64, 3, 1, 1),    # [, 64, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ConvTranspose2d(64, 32, 3, 1, 1),      # [, 32, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ConvTranspose2d(32, 32, 3, 1, 1),      # [, 32, 48, 48]\n",
    "            nn.ConvTranspose2d(32, 16, 3, 2, 1, 1),  # [, 16, 96, 96]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ConvTranspose2d(16, 3, 3, 1, 1),         # [, 3, 96, 96]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoder1 = self.Encoder1(x)\n",
    "        decoder1 = self.Decoder1(encoder1)\n",
    "        encoder2 = self.Encoder2(x)\n",
    "        decoder2 = self.Decoder2(encoder2)\n",
    "        gamma = self.Gamma(x)\n",
    "\n",
    "        img_mid = (x*decoder1)+decoder2\n",
    "        b = img_mid.shape[0]\n",
    "        img_high = torch.stack([img_mid[i,:,:,:]**gamma[i,:] for i in range(b)], dim=0)\n",
    "\n",
    "        return img_high\n",
    "        \n",
    "DAEmodel = DenoiseAutoEncoder()\n",
    "DAEmodel.load_state_dict(torch.load('./last_Epoch_3ae.pth',map_location=\"cpu\"))\n",
    "DAEmodel.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 16552055\n",
      "torch.Size([1, 3, 400, 600])\n"
     ]
    }
   ],
   "source": [
    "#验证模型结果\n",
    "\n",
    "img = torch.Tensor(1, 3, 400, 600)\n",
    "print('total parameters:', sum(param.numel() for param in DAEmodel.parameters()))\n",
    "a = DAEmodel(img)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始转换\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 287/288 [00:00<00:00, 4637.34 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 39/39 [00:00<00:00, 303.40 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 11/11 [00:00<00:00, 190.67 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 416/416 [00:14<00:00, 27.90 ops/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set feature descriptions (these show up as comments in XCode)\n"
     ]
    }
   ],
   "source": [
    "# 直接用coremltools，pytorch_1.12.1 可用\n",
    "import torch\n",
    "import coremltools as ct\n",
    "import coremltools\n",
    "random_input = torch.rand(1, 3, 400, 600) \n",
    "traced_model = torch.jit.trace(DAEmodel, random_input) \n",
    "print(\"开始转换\")\n",
    "model = ct.convert(\n",
    "    model=traced_model,\n",
    "    source=\"pytorch\",\n",
    "    inputs=[ct.ImageType(name=\"input_image\", shape=random_input.shape, scale=1 / 255.0, color_layout=ct.colorlayout.RGB)],\n",
    "    # inputs=[ct.TensorType(name=\"input_tensor\",dtype=np.float32, shape=(1,3,400,600))],\n",
    "    # outputs=[ct.TensorType(name=\"mid_res\"), ct.ImageType(name=\"output_image\",color_layout=ct.colorlayout.RGB)],\n",
    "    outputs=[ct.TensorType(name=\"output_tensor\",dtype=np.float32)],\n",
    "    # outputs=[\"mid_res\",\"output_tensor\"],\n",
    "    minimum_deployment_target=coremltools.target.iOS14, #Currently models that use bilinear upsampling in PyTorch can be converted to CoreML models targeting iOS 14, but not iOS 13.\n",
    "    convert_to=\"neuralnetwork\",\n",
    ") \n",
    "\n",
    "\n",
    "print(\"Set feature descriptions (these show up as comments in XCode)\")\n",
    "# model.input_description[\"input_tensor\"] = \"a (1,3,96,96) shaped tensor transfered from an image\"\n",
    "# model.output_description[\"output_tensor\"] = \"a (1,3,96,96) tensor, *255 and transpose((1,2,0) to convert to an image\"\n",
    "model.author = \"Alyosha\"\n",
    "model.license = \"leigithub1024/xxx\"\n",
    "model.short_description = \"lowlight image enhancement\"\n",
    "model.version = \"1.0\"\n",
    "modelName = \"model.mlmodel\"\n",
    "model.save(modelName)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"将输出结果*255\")\n",
    "# spec = coremltools.utils.load_spec(modelName)\n",
    "# builder = coremltools.models.neural_network.NeuralNetworkBuilder(spec=spec)\n",
    "# # builder.add_elementwise(name=f\"xx\", input_names=[f\"mid_res\"], output_name=f\"mid_res_xx\", mode=\"MULTIPLY\", alpha=255)\n",
    "# builder.add_elementwise(name=f\"multiply_xy_by_two_output_image\", input_names=[f\"output_tensor\"], output_name=f\"output_image\", mode=\"MULTIPLY\", alpha=255)\n",
    "# builder.set_output(output_names=[\"output_image\"], output_dims=[(3,400,600)])\n",
    "# # builder.set_output(output_names=[\"output_image\"], output_dims=[(3,96,96)])\n",
    "\n",
    "# model_spec = builder.spec\n",
    "# coremltools.models.utils.save_spec(model_spec, modelName)\n",
    "\n",
    "# print(model_spec.description.output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"input_image\"\n",
      "type {\n",
      "  imageType {\n",
      "    width: 600\n",
      "    height: 400\n",
      "    colorSpace: RGB\n",
      "  }\n",
      "}\n",
      "] [name: \"output_image\"\n",
      "type {\n",
      "  imageType {\n",
      "    width: 600\n",
      "    height: 400\n",
      "    colorSpace: RGB\n",
      "  }\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 更改模型输出为图像\n",
    "import coremltools\n",
    "import coremltools.proto.FeatureTypes_pb2 as ft \n",
    "spec = coremltools.utils.load_spec(modelName)\n",
    "output = spec.description.output[0]\n",
    "import coremltools.proto.FeatureTypes_pb2 as ft\n",
    "output.type.imageType.colorSpace = ft.ImageFeatureType.RGB\n",
    "output.type.imageType.height = 400\n",
    "output.type.imageType.width = 600\n",
    "output.name = 'output_image'\n",
    "coremltools.utils.save_spec(spec, modelName)\n",
    "print(spec.description.input,spec.description.output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error compiling model: \"Error reading protobuf spec. validator error: Layer '494' of type 925 has 1 inputs but expects at least 2.\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m600\u001b[39m,\u001b[38;5;241m400\u001b[39m))\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# img = img.astype(np.float32) / 255.0\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m out_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_image\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#####输出格式：tensor\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# res = out_dict[\"output_tensor\"] \u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# arr = np.array((res[0]*255), dtype=np.uint8)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# image_array = image_array * 255\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# print(image_array)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m out_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_image\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshow()  \u001b[38;5;66;03m#这里发现输出是全黑的，找到问题了，coreml的那个input格式转换没调好\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/coremltools/models/model.py:545\u001b[0m, in \u001b[0;36mMLModel.predict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/coremltools/models/model.py?line=542'>543</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/coremltools/models/model.py?line=543'>544</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_framework_error:\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/coremltools/models/model.py?line=544'>545</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_framework_error\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/coremltools/models/model.py?line=545'>546</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/coremltools/models/model.py?line=546'>547</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnable to load CoreML.framework. Cannot make predictions.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/coremltools/models/model.py:143\u001b[0m, in \u001b[0;36m_get_proxy_and_spec\u001b[0;34m(filename, compute_units, skip_model_load)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/coremltools/models/model.py?line=139'>140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mNone\u001b[39;00m, specification, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/coremltools/models/model.py?line=141'>142</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/coremltools/models/model.py?line=142'>143</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (_MLModelProxy(filename, compute_units\u001b[39m.\u001b[39;49mname), specification, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/coremltools/models/model.py?line=143'>144</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/coremltools/models/model.py?line=144'>145</a>\u001b[0m     _warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/coremltools/models/model.py?line=145'>146</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou will not be able to run predict() on this Core ML model.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/coremltools/models/model.py?line=146'>147</a>\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m Underlying exception message was: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/coremltools/models/model.py?line=147'>148</a>\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/coremltools/models/model.py?line=148'>149</a>\u001b[0m         \u001b[39mRuntimeWarning\u001b[39;00m,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/torchversion_1_12_1/lib/python3.10/site-packages/coremltools/models/model.py?line=149'>150</a>\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error compiling model: \"Error reading protobuf spec. validator error: Layer '494' of type 925 has 1 inputs but expects at least 2.\"."
     ]
    }
   ],
   "source": [
    "# 在这个网站验证模型结构：https://netron.app/\n",
    "# 验证模型能不能跑\n",
    "# Use PIL to load and resize the image to expected size\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "model = coremltools.models.MLModel(modelName)\n",
    "\n",
    "#####输入格式：tensor\n",
    "# example_image = Image.open(\"1.jpg\").resize((96, 96))\n",
    "# img = cv2.imread('./1.jpg')\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# img = cv2.resize(img, (96, 96),interpolation = cv2.INTER_LINEAR)\n",
    "# img = img.astype(np.float32) / 255.0\n",
    "# # mean = np.array([0.485, 0.456, 0.406])\n",
    "# # val = np.array([0.229, 0.224, 0.225])\n",
    "# # img = (img - mean) / val\n",
    "# print(\"gemfield debug img shape1: \",img.shape)\n",
    "# img= img.astype(np.float32)\n",
    "# img = img.transpose((2,0,1))\n",
    "# print(\"gemfield debug img shape2: \",img.shape)\n",
    "# img = np.expand_dims(img,axis=0)\n",
    "# print(\"gemfield debug img shape3: \",img.shape)\n",
    "# out_dict = model.predict({\"input_tensor\": img})\n",
    "\n",
    "\n",
    "##### 输入格式：image\n",
    "img = Image.open('1.jpg')\n",
    "img = img.resize((600,400))\n",
    "# img = img.astype(np.float32) / 255.0\n",
    "out_dict = model.predict({\"input_image\": img})\n",
    "\n",
    "\n",
    "\n",
    "#####输出格式：tensor\n",
    "# res = out_dict[\"output_tensor\"] \n",
    "# arr = np.array((res[0]*255), dtype=np.uint8)\n",
    "# print(\"arr shape:\", arr.shape)\n",
    "# arr = arr.transpose((1,2,0))\n",
    "# print(\"transposed_arr shape\",arr.shape, arr)\n",
    "# image = Image.fromarray(arr)\n",
    "# image.show()\n",
    "\n",
    "\n",
    "\n",
    "##### 输出格式：图像\n",
    "# print(out_dict)\n",
    "# output_pil_image = out_dict[\"output_image\"]\n",
    "# image = output_pil_image.convert(\"RGB\")\n",
    "# image_array = np.array(image)\n",
    "# image_array = image_array * 255\n",
    "# print(image_array)\n",
    "out_dict['output_image'].show()  #这里发现输出是全黑的，找到问题了，coreml的那个input格式转换没调好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##尝试使用 pth => onnx => mlmodel\n",
    "#转换为onnx\n",
    "dummy_input = torch.rand(1, 3, 96, 96) #这里高宽可能是颠倒的\n",
    "input_names = [\"gemfield_in\"]\n",
    "output_names = [\"gemfield_out\"]\n",
    "torch.onnx.export(DAEmodel,\n",
    "                  dummy_input,\n",
    "                  \"syszux_scene.onnx\",\n",
    "                  verbose=True,\n",
    "                  input_names=input_names,\n",
    "                  output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#验证使用onnx推理\n",
    "\n",
    "import cv2\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "session = onnxruntime.InferenceSession(\"./syszux_scene.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[1].name\n",
    "input_shape = session.get_inputs()[0].shape\n",
    "print(\"gemfield debug required input shape\", input_name,output_name,input_shape)\n",
    "\n",
    "img = cv2.imread('./10.png')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#INTER_NEAREST, INTER_LINEAR, INTER_AREA, INTER_CUBIC\n",
    "img = cv2.resize(img, (96, 96),interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "img = img.astype(np.float32) / 255.0\n",
    "\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "val = np.array([0.229, 0.224, 0.225])\n",
    "# img = (img - mean) / val\n",
    "# print(img)\n",
    "\n",
    "print(\"gemfield debug img shape1: \",img.shape)\n",
    "img= img.astype(np.float32)\n",
    "img = img.transpose((2,0,1))\n",
    "#img = img.transpose((2,1,0))\n",
    "print(\"gemfield debug img shape2: \",img.shape)\n",
    "img = np.expand_dims(img,axis=0)\n",
    "print(\"gemfield debug img shape3: \",img.shape)\n",
    "\n",
    "res = session.run([output_name], {input_name: img})\n",
    "arr = np.array((res[0][0]*255), dtype=np.uint8)\n",
    "print(\"arr shape:\", arr.shape)\n",
    "\n",
    "arr = arr.transpose((1,2,0))\n",
    "print(\"transposed_arr shape\",arr.shape, arr)\n",
    "image = Image.fromarray(arr)\n",
    "image.save('image.jpg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#转换为mlmodel\n",
    "#报错：No module named 'coremltools.converters.nnssa'，换了好几个环境也无法解决\n",
    "from onnx_coreml import convert\n",
    "model = convert(model='syszux_scene.onnx',minimum_ios_deployment_target='13')\n",
    "model.save('syszux_scene.mlmodel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  4],\n",
      "        [10,  8]])\n",
      "tensor([[ 4,  4],\n",
      "        [10,  8]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create two tensors\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[2, 0], [1, 2]])\n",
    "\n",
    "# Compute the dot product of a and b along the last axis of a and the second-to-last axis of b\n",
    "result = torch.tensordot(a, b, dims=([-1], [-2]))\n",
    "\n",
    "print(result)  \n",
    "\n",
    "result = torch.matmul(a,b)\n",
    "print(result) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.8689, -5.2327, -6.6254,  ..., -3.8065, -1.6950,  4.2786],\n",
      "         [-1.2535, -5.0202, -2.4007,  ..., -3.3875,  0.0727,  6.7865],\n",
      "         [ 3.8709,  3.2595, -4.3169,  ..., -4.2282, -0.1526,  3.1204],\n",
      "         ...,\n",
      "         [ 5.9847,  7.4674,  3.5788,  ...,  4.4165,  0.4777, -7.7185],\n",
      "         [ 5.6143,  0.0232, -0.8350,  ...,  6.3332,  0.8166,  1.1386],\n",
      "         [-2.9651, -1.1399,  3.1023,  ...,  6.5655,  1.4220,  2.8216]]]) torch.Size([1, 240000, 16])\n",
      "tensor([[[ 1.8689, -5.2327, -6.6254,  ..., -3.8065, -1.6950,  4.2786],\n",
      "         [-1.2535, -5.0202, -2.4007,  ..., -3.3875,  0.0727,  6.7865],\n",
      "         [ 3.8709,  3.2595, -4.3169,  ..., -4.2282, -0.1526,  3.1204],\n",
      "         ...,\n",
      "         [ 5.9847,  7.4674,  3.5788,  ...,  4.4165,  0.4777, -7.7185],\n",
      "         [ 5.6143,  0.0232, -0.8350,  ...,  6.3332,  0.8166,  1.1386],\n",
      "         [-2.9651, -1.1399,  3.1023,  ...,  6.5655,  1.4220,  2.8216]]]) torch.Size([1, 240000, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn(1,240000,16)\n",
    "b = torch.randn(16,16)\n",
    "\n",
    "result = torch.tensordot(a, b, dims=([-1], [-1]))\n",
    "print(result, result.shape)  \n",
    "\n",
    "result1 = torch.matmul(a,b.t())\n",
    "print(result1, result1.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13, 18],\n",
      "        [17, 24],\n",
      "        [21, 30]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "y = torch.tensor([[1, 2], [3, 4]])\n",
    "# print(torch.matmul(x,y))\n",
    "print(torch.tensordot(x, y, dims=([-2], [-2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#验证coreML推理"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8784ac5ad4e3e833f4f0d06ce52de7a7ad21b04fbe5fdfd3f9003e835db4d55"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('mxy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
